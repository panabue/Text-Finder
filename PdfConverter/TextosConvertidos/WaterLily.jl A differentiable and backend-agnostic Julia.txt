WaterLily.jl: A differentiable and backend-agnostic Julia
solver to simulate incompressible viscous flow and dynamic
bodies
Gabriel D. Weymouth1 and Bernat Font1,2,∗
1Faculty of Mechanical Engineering, Delft University of Technology, Delft, Netherlands
2Barcelona Supercomputing Center, Barcelona, Spain
July 24, 2024
Abstract
Integrating computational fluid dynamics (CFD) software into optimization and machine-learning frame-
works is hampered by the rigidity of classic computational languages and the slow performance of more
flexible high-level languages. In this work, we introduce WaterLily.jl: an open-source incompressible viscous
flow solver written in the Julia language. An immersed boundary method is used to enforce the effect of
solid boundaries on flow past complex geometries with arbitrary motions.
The small code base is mul-
tidimensional, multiplatform and backend-agnostic (serial and multithreaded CPU, and GPU execution).
Additionally, the dynamically typed language allows the solver to be fully differentiable using automatic
differentiation. The computational time per time step scales linearly with the number of degrees of freedom
(DOF) on CPUs, and we measure up to a 200x speed-up using CUDA kernels resulting in a cost of 1.44
nanoseconds per DOF and time step. This leads to comparable performance with low-level CFD solvers
written in C and Fortran on research-scale problems, opening up exciting possible future applications on
the cutting edge of machine-learning research.
Keywords: computational fluid dynamics; heterogeneous programming; Cartesian-grid methods; Julia
WaterLily.jl repository: https://github.com/WaterLily-jl/WaterLily.jl
Manuscript repository: https://github.com/WaterLily-jl/WaterLily.jl_CPC_2024
1
Introduction
During the last decade, the computational fluid dynamics (CFD) community has embraced the surge of machine
learning (ML) and the new developments in hardware architecture, such as general-purpose graphics-processing
units (GPUs). Hence, classic CFD solvers based on low-level programming languages (C, Fortran) and CPU
memory-distributed execution are now adapted to accommodate these new tools. On one hand, the integration
of high-level ML libraries and low-level CFD solvers is not straight-forward, aka. the two-language problem
(Churavy et al., 2022). When deploying an ML model online with the CFD solver, data exchange is often
performed at disk level, significantly slowing down the overall runtime because of disk read and write operations.
An improved way to exchange data is performed through memory, either using Unix sockets (Rabault et al.,
2019; Font et al., 2021), message-passing interface (MPI) (Guastoni et al., 2023), or an in-memory distributed
database (Kurz et al., 2022; Font et al., 2024), which increases the software complexity. On the other hand,
porting classic CFD solvers to GPU is also a non-trivial task which often requires the input and expertise of
GPU vendors (Romero et al., 2022). Still, the CFD community has been an active asset in this transition, and
it currently offers a rich variety of open-source multi-GPU solvers as summarized in Tab. 1.
In this context, Julia (Bezanson et al., 2017) emerges as an open-source, compiled, dynamic, and compos-
able programming language specifically designed for scientific computing which can help tackle such software
challenges. High-level libraries and low-level code can co-exist without compromising computing performance.
Moreover, its excellent meta-programming capabilities, dynamic types, and multiple-dispatch strategy maxi-
mizes code re-usability. A great example of this is the KernelAbstractions.jl library (Churavy et al., 2023),
which enables writing heterogeneous kernels for different backends (multithreaded CPU, NVIDIA, AMD, and
others) in a single framework. Julia has been also tested in many HPC systems, and the reader is referred to
Churavy et al. (2022) for a comprehensive review.
In this work, we introduce a new CFD solver with heterogeneous execution written in Julia, namely Wa-
terLily.jl. Differently to most of the solvers detailed in Tab. 1, WaterLily profits from a dynamically-typed
∗b.font@tudelft.nl
1
arXiv:2407.16032v1  [physics.flu-dyn]  22 Jul 2024
Name
Application
Method
Language
CaNS
Costa (2018)
Incompressible canonical flows on
rectilinear grids
FDM
Fortran/OpenACC
GALÆXI
Kempf et al. (2024)
Compressible flows on unstructured grids
DG
CUDA-Fortran
nekRS
Fischer et al. (2022)
Incompressible flows on unstructured grids
SEM
C++/OCCA
Oceananigans.jl
Ramadhan et al. (2020)
Geophysical flows
FVM
Julia
OpenSBLI
Lusher et al. (2021)
Code-generation system for compressible
flows on structured grids
FDM
Python +
CUDA/OpenCL
PyFR
Witherden et al. (2015)
Compressible/incompressible flows on
unstructured grids
FR
Python + C/OpenMP,
CUDA, OpenCL
RHEA
Jofre et al. (2023)
Compressible flows on rectilinear grids
FDM
C++/OpenACC
SOD2D
Gasparino et al. (2024)
Compressible/incompressible flows on
unstructured grids
SEM
Fortran/OpenACC
STREAmS
Bernardini et al. (2021)
Compressible canonical wall-bounded flows
on rectilinear grids
FDM
CUDA-Fortran
Table 1: Examples of multi-GPU open-source CFD solvers.
Methods are abbreviated as: finite difference
method (FEM), discontinuous Galerkin (DG), spectral element method (SEM), finite volume method (FVM),
and flux reconstruction (FR).
language that allows to efficiently implement performance-critical code in a compact and uniform framework,
noting that the solver codebase is less than 1000 lines of code. This results in a fully-differentiable CFD solver
that is easy to maintain and that can run in CPU or GPU architectures of different vendors without compro-
mising performance. With this, the numerical methods and software design are respectively reported in sections
§2 and §3. The solver is benchmarked and validated in §4. Two different test cases showcasing notable features
of the solver are shown in §5. Finally, conclusions and expectations are presented in §6.
2
Numerical methods
WaterLily uses the boundary data immersion method (BDIM) to simulate the fluid flow around immersed bodies
(Weymouth and Yue, 2011; Maertens and Weymouth, 2015; Lauber et al., 2022). The preceding references
give the precise mathematical formulation, as well as detailed validation of the immersed-boundary method’s
accuracy. To summarize the approach, the momentum equation defined over the fluid domain
˙F : ˙ui = −p,i −(uiuj),j + νui,jj
∀i, j ∈1 . . . n
(1)
is integrated in time and convolved with a prescribed body velocity defined over the solid domain
B : ui = Vi
∀i ∈1 . . . n
(2)
resulting in a single meta-equation valid over the whole space. In these equations, ui are the velocity components
in a n-dimensional flow, p is the pressure scaled by the fluid density, ν is the fluid viscosity, Vi is the body
velocity, indices after commas indicate spacial derivatives, and summation is used over repeated indices. As
with these equations, WaterLily can be applied to simulations of any number of dimensions n, although we
typically restrict applications to 2D and 3D flows.
The immersed-boundary thickness ϵ defines the region directly affected by the prescribed body velocities,
but the flow inside this region still obeys the fluid dynamic equations with second-order accuracy (Maertens
and Weymouth, 2015). The transition between these two regions is defined by the properties of the immersed
surface, specifically the signed-distance d and normal ˆn from any point in space to the closest surface point.
This, along with the body velocity Vi, defines the local meta-equation.
WaterLily implements the governing equation using a finite-volume approach on a uniform Cartesian grid
with staggered velocity-pressure variable placement. Since all grid cells are identical, no grid information is
stored. Second-order central differences are used for the pressure and diffusion terms, while a flux-limited Quick
scheme is used on the convective term. While explicit turbulence models have been used for specific projects,
the core WaterLily package is model-free, making it an implicit Large Eddy Simulation (iLES) solver (Margolin
et al., 2006).
2
Finally, the momentum equation is integrated in time using an explicit predictor-corrector update scheme
(Lauber et al., 2022). The velocity is restricted to be incompressible (divergence-free, ui,i = 0) using a pressure
projection scheme at each step. The resulting Poisson equation has spatially varying coefficient in the presence
of immersed boundaries, and it is solved using a geometric multi-grid method (Weymouth, 2022). The time step
is adapted automatically to control the maximum Courant—Friedrichs—Lewy (CFL) number in the domain.
3
Software design
Julia’s flexible and fast programming capabilities enabled the implementation of WaterLily to have many special
features in a minimal codebase. For example, automatic differentiation (AD) is used to define all the properties
of the immersed geometry from a user-defined signed-distance function and a coordinates-mapping function.
Moreover, the whole solver is also differentiable based on AD. This allows solving optimization problems related
to the immersed body with reduced computational cost compared to a finite-difference/sampling approach. In
addition, AD can also be used to develop accelerated data-driven GMG methods as demonstrated in Weymouth
(2022).
The most important Julia features for implementing the solver to run on heterogeneous backends are (i)
the dynamic typing system, (ii) the meta-programming capabilities, and (iii) the rich open-source packages.
Multiple-dispatch enables simple functions (such as broadcasting or reductions operations on arrays) to be
written at a high-level by the user, while intermediate Julia libraries, and ultimately the compiler, will specialize
the code for efficient execution on a particular architecture (CPU or GPU). For more specialized tasks, WaterLily
uses Julia’s meta-programming features to generate code that produces an individual kernel for the specific task.
The kernel can be used to offload the computational work into a GPU, or to run it in a multithreaded CPU
environment depending on the available system architecture. As an example, the gradient of the n-dimensional
pressure field p is applied to the velocity field u as follows


for i in 1:n
# apply pressure gradient
@loop u[I, i] -= c[I, i] * (p[I] - p[I -
∂(i)]) over I in inside(p)
end


where ∂(i) is a function defining a Cartesian index step in the direction i, c are the coefficients in the
pressure-Poisson matrix arising from the discretization scheme, and inside(p) provides the range of Cartesian
indices I in the pressure field to loop over (excluding ghost cells). For example, if size(p) == (10, 10),
then inside(p) yields a range of CartesianIndices((2:9, 2:9)). When applying the @loop macro to this
expression, the following kernel is produced based on the KernelAbstractions.jl package API (Churavy et al.,
2023)


@kernel function kern_(u, c, p, i, @Const(I0)) # automatically generated kernel
I = @index(Global, Cartesian)
I += I0
@fastmath @inbounds u[I, i] -= c[I, i] * (p[I] - p[I -
∂(i)])
end


which is subsequently launched with the auto-generated call


kern_(get_backend(u))(u, c, p, i, inside(p)[1] - oneunit(inside(p)[1]), ndrange=
size(inside(p)))


Note that @kernel, @index, @Const and get_backend are part of the KernelAbstractactions.jl API and
ultimately generate the appropriate kernel based on the backend inferred by get_backend(u). Also note that
a Cartesian-index based parallelization across the global memory is used, and that the @Const(I0) argument
passes the ghost-cell offset information into the kernel. The workgroup size for the parallelization of the range of
Cartesian indices (ndrange) is automatically inferred base on the size of each dimension in ndrange. Moreover,
the backend of the working arrays, such as u or p, is specified by the user through the mem (for memory) keyword
argument when creating a Simulation object. Hence, with a simple flag, the CFD simulation can be run on
a CPU or a GPU from different vendors. Currently, WaterLily has been successfully tested on both NVIDIA
and AMD GPUs. Similarly, the precision of the simulation is specified with the keyword argument T, which for
example can be set to Float32 (single) or Float64 (double) precision.
As hinted, the main component in WaterLily is the Simulation type, which holds information about the
fluid through the Flow type, and the immersed body (or bodies) through the AutoBody type. Hence, to set up
a simulation, the user must specify the size of the Cartesian grid as well as other optional properties such as
characteristic length and velocity, fluid viscosity, and type of boundary conditions (slip by default, otherwise a
3
(a) TGV
(b) Sphere
(c) Moving cylinder
Figure 1: Time to run 100 time steps in single precision for the Taylor–Green vortex (left), fixed sphere (center),
and moving cylinder (right) cases at different grid sizes. The CPU execution comprises multiple number of
threads from single thread (serial) to 16 threads (multithreading). The speed-up for each case with respect to
the serial execution (CPUx1) is shown above each bar. The speed-up is computed as time(CPUx1)/time(X).
The benchmarks have been run on an accelerated node of the Marenostrum5 supercomputer using Intel Xeon
Platinum 8460Y @ 2.3GHz cores and an NVIDIA Hopper H100 64GB HBM2 GPU. Results obtained on Julia
version 1.10.
Figure 2: Cost, defined as the execution time per grid DOF and time step, on the different cases and grid levels
measured on the GPU backend. Data resulting from Fig. 1 benchmarks (more details in that caption).
convective outlet or a periodic condition can be selected too). On the other hand, the AutoBody type holds the
signed-distance function as well as the coordinates mapping for moving boundaries. More detailed examples on
how to set up a simulation are available in §5.
4
Benchmark and validation
The performance of the solver is assessed on three different cases with increasing level of complexity: the Taylor–
Green vortex (TGV) at Re = 1600, flow past a fixed sphere at Re = 3700, and flow past a moving circular
cylinder at Re = 1000. Note that the cases range from a flow free of solid boundaries to a flow containing a
dynamic body. The TGV case consists of a developing flow transitioning to turbulence in a L3 triple-periodic
cubic domain. The initial condition for the velocity vector field ⃗u0 is prescribed as
u0 = −U sin(κx) cos(κy) cos(κz)
(3)
v0 = U cos(κx) sin(κy) cos(κz)
(4)
w0 = 0,
(5)
where U = 1 is the characteristic velocity and κ = 2π/L is the wavenumber. Similarly to Dairay et al. (2017),
we use the half-domain defined by the characteristic length as the effective computational domain, and apply
4
6
94
(a) TGV
18
4
78
(b) Sphere
23
4
73
accelerate!
BC!
CFL!
conv_diff!
measure!
project!
scale_u!
(c) Moving cylinder
Figure 3: Kernel timings distribution for the 3rd-level grids of the different test cases. Timings are measured as
the median value of the kernel execution time for 1000 time steps, noting that each kernel can be called more
than once for each time step (ie. predictor-corrector scheme). The following convention applies; scale_u!:
scalar operation that scales the velocity field; project!: pressure-Poisson equation solver; measure!: coordi-
nates mapping for a moving solid boundary; conv_diff!: convection-diffusion computation; CFL!: time-step
prediction; BC! boundary conditions; accelerate!: adds a uniform source term to accelerate the background
flow. We note that BC! accounts for all the following boundary-conditions-related subroutines in the codebase:
BC!, BDIM!, BCTuple, exitBC!, where the latter implements a convective outlet, and it is the most expensive
boundary-condition kernel. Tests are conducted using single precision in an NVIDIA GeForce RTX 4060 laptop
GPU. During the tests, 99.9% of the time is spent in kernel execution while only 0.1% is spent on device-to-host
memory-copy calls (related to the pressure solver). Results obtained on Julia version 1.10.
symmetry boundary conditions to lower the cost of the simulation. To test different grid resolutions, we select
L =

26, 27, 28, 29	
resulting into grids of 0.26, 2.10, 16.78, and 134.22 million of degrees of freedom (DOF),
respectively. With respect to the sphere case, its diameter (D) is taken as the characteristic length, and a
16D ×6D ×6D domain is defined for increasing resolutions of D =

23, 24, 25, 26	
resulting in 0.29, 2.36, 18.87,
and 150.99 million DOF, respectively. For the circular cylinder, the diameter is again taken as the characteristic
length in a 12D × 6D × 2D domain and resolutions of L =

24, 25, 26, 27	
resulting in 0.59, 4.72, 37.75, 301.99
million DOF grids. We note that the finer cylinder grid consumes 39 GB of memory using single precision,
and hence can be fitted in the NVIDIA Hopper H100 64GB used for benchmarking. Both the sphere and
cylinder cases are initialized with a uniform flow condition ⃗u0 = (U, 0, 0), where U = 1. For the sphere case,
slip conditions are applied on the lateral boundaries, while a periodic boundary condition is applied on the
spanwise direction of the cylinder case. A convective outlet condition is imposed on the downstream plane of
both cases.
The benchmark of the three cases is measured by timing the execution of 100 time steps using different
backends, as displayed in Fig. 1. On the CPU backend, it is noted that increasing the number of threads
enables faster simulations in a linear trend that slowly stagnates to a factor of x8 speed-up for the TGV and
sphere cases, and x6 on the cylinder case, on their respective finer grid. Except for the very small grids, the
CPU backend does not yield a larger speed-up when increasing the grid size. In contrast, the speed-up of the
GPU with respect to the serial CPU execution is greatly increased as the GPU VRAM is filled, reaching a peak
of x200 speed-up factors for the TGV and sphere cases. The effect of improved performance when maximizing
the GPU memory occupancy is also observed in other CFD codes (Kempf et al., 2024). Indeed, the overhead
cost associated to launching a kernel becomes less important when maximizing the computational work of the
kernel by increasing the grid size (and hence the required parallel workload). This trend of the GPU backend
becomes more clear in Fig. 2, where the cost (ie. execution time per DOF and time step) significantly decreases
with increasing grid size. Qualitatively, the cost transitions from being kernel-launch bounded to workload
bounded for grids larger than 10M DOF.
The profiling of the solver is conducted on an NVIDIA GeForce RTX 4060 laptop GPU backend by timing
the main kernels of the time-stepping routine using the NVTX.jl profiling package (Byrne and Besard, 2024)
(a wrapper for the NVIDIA Tools Extension Library, NVTX). The kernel profiling of the (median) time-step
execution time is displayed in Fig. 3 for the different cases. Noticeably, the project! kernel, ie. the pressure
solver routine, consumes most of the execution time for all cases, up to 94% on the TGV case. It is worth
noting that a preconditioned conjugate gradient smoother (PCG) is employed in the geometric multi-grid
solver. Computing dot products (array reductions) on a GPU is known to be rather inefficient because of their
low arithmetic intensity, and the PCG solver contains several reduction operations. Furthermore, imposing
boundary conditions (the BC! kernel) also becomes expensive on the cases including solid boundaries. Again,
the overhead associated to the kernel launch is non-negligible when the computational cost is small, which is
indeed the case when processing the boundary ghost cells (two-dimensional slices in a three-dimensional array).
5
Figure 4: Taylor–Green vortex (TGV) temporal evolution of kinetic energy (left) and enstrophy (right). Direct
numerical simulation (DNS) data from Dairay et al. (2017) is used as reference.
Figure 5: Time-averaged drag coefficient measured on the sphere at Re = 3700 for different resolutions (cells per
diameter). The time-averaged metric is integrated over 300 convective time units (CTU, tU/L) after discarding
the first 100 CTU used to reach the statistically-steady state of the wake.
With respect to the memory transfer between host (CPU) and device (GPU), the profiling shows that 99.9%
of the time is spent in kernel launching and computing, while only 0.1% is spent in memory-copy (memcpy)
operations.
The validation of the solver is performed for the TGV and the sphere cases. A grid convergence of the
TGV case based on temporal evolution of kinetic energy and enstrophy is displayed in Fig. 4. The finest grid
consists of 5123 cells spanning the symmetric subdomain (1/8th) of the triple periodic box, similarly to the
direct numerical simulation (DNS) reference data by Dairay et al. (2017). The fine grid results greatly match
the DNS reference data in which this same resolution was used. With respect to the sphere case, a domain
of 7D × 3D × 3D is considered for validation. The reason for this is to allow for a greater resolution of the
boundary layer, which would not be feasible with the domain used for benchmarking (16D × 6D × 6D). With
this, the grids tested for validation contain 88, 128, and 168 cells per diameter, totalling 43M, 132M, and 299M
DOF, respectively. The minimum boundary layer thickness around the sphere at Re = 3700 is approximately
δ/D = 0.02 (Capuano et al., 2023), and DNS studies such as Rodriguez et al. (2011) fit 12 grid points within
the boundary layer thickness. In this case, the finest grid has a resolution of h/D = 0.006 approximately, which
means that only 3 grid points are used to represent the boundary layer. Hence, it is important noticing that
the current Cartesian-mesh method using constant spacing requires vast resources to fully resolve the boundary
layer of the immersed bodies. Still, a converged time-averaged drag coefficient of CD = 0.35 is found for all the
tested grids, which correctly matches the LES data from Yun et al. (2006) and is only within the 10% error of
the DNS data from Rodriguez et al. (2011).
6
(a) Vorticity field
(b) Scaled power history
(c) Optimization process
Figure 6: Controlled flow over a static cylinder using spinning cylinders in the wake. The small cylinder (purple)
has scale spin velocity ξ, driving the flow to be symmetry and steady after an initial transient, as measured by
the vorticity (a) and the scaled power coefficient CP (b). The net propulsive efficiency is optimized using the
differentiable solver (c).
5
Sample applications
Three applications are selected to demonstrate the capability of the package to analyze general fluid flows. The
examples also showcase the advantages of a differentiable backend-agnostic Cartesian-grid solver.
5.1
Optimized control cylinders
The first example will be optimizing the controlled 2D flow around a circle using a pair of small spinning circles
placed 120 degrees relative to the inflow direction, Fig. 6a. Experimental and numerical studies of this system
have show the capability of the spinning cylinders to control the flow over the large circle Schulmeister et al.
(2017), establishing a steady symmetric wake, reducing the system drag and even producing a net thrust as the
rotation rate is increased.
The system is described by a few dimensionless ratios: Re = UD/ν the Reynolds number based on the large
circle diameter and inflow velocity, d/D the scaled diameter of the control circle, g/D the gap between the large
circle and the control circle, and ξ = 1
2dΩ/U the control circle scaled surface speed. This system is simulated
with WaterLily using the values of Re = 500, d/D = 0.15, g/D = 0.05 with grid resolution D/h = 96. The
domain is sized to 6D × 2D taking advantage of the known symmetry of the flow by using a symmetry plane
and only modelling the upper half of the full domain. The entire differentiable simulation is defined with the
simple script
7
Figure 7: Flow induced by a pulsing jellyfish geometry visualized by vorticity magnitude at equally spaced
intervals over a cycle.


rot(θ) = [cos(θ) -sin(θ); sin(θ) cos(θ)] # rotation matrix
function drag_control_sim(ξ; D=96, Re=500, d_D=0.15f0, g_D=0.05f0)
# set up big cylinder
C, R, U = [2D, 0], D÷2, 1
big = AutoBody((x, t) -> √sum(abs2, x), x - C) - R) # signed-distance
function
# set up small control cylinder
r = d_D * R
c = C + (R + r + g_D * D) * [1 / 2, √3 / 2]
small = AutoBody(
(x, t) -> √sum(abs2, x) - r,
# signed-distance function
(x, t) -> rot(ξ * U * t / r) * (x - c) # center and spin!
)
# set up simulation
Simulation((6D, 2D), (U, 0), D; ν=U * D / Re, body=big + small, T=typeof(ξ))
end


This example demonstrates that WaterLily can combine AutoBody types based on the arithmetic of signed-
distance functions. The two big and small circles are defined with a line of code each and combined trivially
with body = big + small. It also demonstrates that the variable ξ is used to set the types employed for
the simulation. This allows easy switching between any floating point precision, but it also allows automatic
differentiation to be applied to the solver as a whole by running the code with a T = Dual data-type holding
the value and derivative simultaneously (Revels et al., 2016).
We use the differentiable solver to maximize the scaled propulsive power CP = FU/ρdc3 where F is the net
thrust force on the system. This metric is proportional to the propulsive efficiency since ρdc3 scales with the
power required to rotate the control cylinders (Schulmeister et al., 2017). The time history of CP is plotted for
a few values of ξ in Fig. 6b, demonstrating that only a few convective cycles are required to reach steady state,
as well as the control authority of ξ over the propulsive power.
We optimize ˆξ = argmax CP (ξ) at time t∗= tU/L = 2 using Davidon’s method (Davidon, 1991), which
evaluates CP and its derivative ∂CP /∂ξ at points bracketing an optimum, using inverse cubic interpolation
to iteratively restrict the interval. Both the value and derivative of the power are computed simultaneously
using dual numbers, at a cost only 80% larger than evaluating the function alone. Fig. 6c shows the resulting
evaluation history starting with the interval ξ = [3, 8], leading to the optimum ˆξ ≈6.26 in a few iterations.
Rates above this optimum produce more net thrust, but require excessive rotation rates to produce.
5.2
Deforming and dynamic geometries
The final two examples showcase the solver’s ability to handle more complex geometries with ease. The first is
a pulsing jellyfish-inspired geometry, and the second is a whale tail-inspired geometry. Both of these cases are
fast enough to simulate on a laptop GPU for live demonstrations at reduced resolution.
8
Figure 8: Flow induced by a flapping whale tail geometry visualized by vorticity magnitude at equally spaced
intervals over a cycle using chord resolution L = 96 and sweep s = 10.


function jelly(p=5; Re=500, mem=CuArray, U=1)
# Define simulation size, geometry dimensions, & viscosity
n = 2ˆp
R = 2n / 3
h = 4n - 2R
ν = U * R / Re
# Motion functions
ω = 2U / R
A(t) = 1 .- [1, 1, 0] * 0.1 * cos(ω * t)
B(t) = [0, 0, 1] * ((cos(ω * t) - 1) * R / 4 - h)
C(t) = [0, 0, 1] * sin(ω * t) * R / 4
# Build jelly from a mapped sphere and plane
sphere = AutoBody(
(x, t) -> abs(√sum(abs2, x) - R) - 1, # sdf
(x, t) -> A(t) .* x + B(t) + C(t)
# map
)
plane = AutoBody((x, t) -> x[3] - h, (x, t) -> x + C(t))
body =
sphere - plane
# Return initialized simulation
Simulation((n, n, 4n), (0, 0, -U), R; ν, body, mem, T=Float32)
end


The bell of the jellyfish is constructed with more AutoBody-arithmetic, in this case taking the difference
of a hollow sphere with an oriented plane.
This geometry is made to pulse by mapping the coordinates
harmonically in the radial and transverse directions. While the geometry maintains a roughly constant solid
volume throughout the pulse, small deviations are handled gracefully by the solver. Fig. 7 shows equally spaced
snapshots of the geometry and resulting flow throughout the cycle. Each cycle generates a strong propulsive
vortex ring which breaks up as it propagates away, in qualitative agreement with experimental studies such as
Dabiri et al. (2005).


function whale(s=10; L=24, U=1, Re=1e4, T=Float32, mem=CuArray)
pnts = [
0 40 190
200
190
170
100
0 -10 0
0
0
8s 8s+40 5s+70 5s+50 5s+70 100
80 0
]
planform = BSplineCurve(reverse(pnts), degree=3)
function map(x, t)
θ, h = π / 6 * sin(U * t / L), 1 + 0.5cos(U * t / L)
Ry = SA[cos(θ) 0 sin(θ); 0 1 0; -sin(θ) 0 cos(θ)]
Ry * 100 * (x / L - SA[0.75, 0, h])
end
body = PlanarParametricBody(planform, (0, 1); map, mem)
Simulation((5L, 3L, 2L), (U, 0, 0), L; U, ν=U * L / Re, body, T, mem)
end


The final example of the whale tail is a planar membrane define using the ParametricBodies.jl package
(Weymouyh and Lauber, 2023). The planform is defined by a set of points which are interpolated using a cubic
spline. The sweep of the wing is adjustable with the input parameter s, allowing parametric geometry studies
to be carried out with ease, as in Zurman-Nasution et al. (2021). The 3D distance function and normal to
9
this planar membrane are then evaluated using a parametric root-finding method to immerse the geometry in
the simulation as with the examples above. Harmonic pitch and heave motion are used to flap the tail. Fig. 8
shows equally spaced snapshots of the geometry and resulting flow throughout the cycle, matching the vortex
structures found in previous works (Zurman-Nasution et al., 2021).
6
Conclusions
In this work, an incompressible viscous flow solver written in the Julia language has been presented, namely Wa-
terLily.jl. With a minimal codebase (approximately 1000 lines of code), WaterLily implements an n-dimensional
CFD solver based on a Cartesian-grid finite-volume method which is able to handle arbitrary moving bodies
through the boundary data immersion method. Using Julia’s high-level libraries such as KernelAbstractions.jl
and ForwardDiff.jl, the solver is able to run in any architecture (serial CPU, multithread CPU, and GPU of
different vendors), and it offers full differentiability based on automatic differentiation (AD).
Based on three different cases (TGV, fixed sphere, and moving cylinder), benchmarking results show that
execution on a modern GPU can yield up to a 200 speed-up factor compared to serial CPU execution. Profiling
on the GPU backend shows that the pressure solver is the most critical component of the time-stepping routine,
and validation results demonstrate the accuracy of the solver on the TGV and sphere cases.
In addition, we provide an example of the AD capabilities of the solver with the classical rotating cylinder
control problem.
The optimal spinning rate of the small cylinder controlling the wake of the large static
cylinder is found with a few optimization steps based on the scaled propulsive power derivative. Last, the
possibility of simulating complex dynamic bodies is showed with a jellyfish-inspired geometry that heaves while
also expanding and contracting, and a parametrically-defined flapping whale tail. Future work will focus on
the parallelization of the solver at the distributed-memory level using the message passing interface (MPI)
standard, the inclusion of multiphase flow simulation through the volume-of-fluid method, and the continuous
improvement of the performance of the solver.
7
Acknowledgements
The authors acknowledge the Barcelona Supercomputing Center for awarding access to the MareNostrum5 sys-
tem. The authors also acknowledge Dr. Valentin Churavy for creating KernelAbstractions.jl and his continued
support, and Dr. Lucas Gasparino for fruitful discussions and initial tests on his personal GPU.
References
Bernardini, Matteo, Modesti, Davide, Salvadore, Francesco, and Pirozzoli, Sergio (2021). STREAmS: A high-
fidelity accelerated solver for direct numerical simulation of compressible turbulent flows. Computer Physics
Communications, 263:107906. doi:10.1016/j.cpc.2021.107906.
Bezanson, Jeff, Edelman, Alan, Karpinski, Stefan, and Shah, Viral B. (2017). Julia: A Fresh Approach to
Numerical Computing. SIAM Review, 59(1):65–98. doi:10.1137/141000671.
Byrne, Simon and Besard, Tim (2024). NVTX.jl. https://github.com/JuliaGPU/NVTX.jl.
Capuano, Francesco, Beratlis, Nikolaos, Zhang, Fengrui, Peet, Yulia, Squires, Kyle, and Balaras, Elias (2023).
Cost vs Accuracy: DNS of turbulent flow over a sphere using structured immersed-boundary, unstruc-
tured finite-volume, and spectral-element methods. European Journal of Mechanics - B/Fluids, 102:91–102.
doi:10.1016/j.euromechflu.2023.07.008.
Churavy, Valentin, Dilum Aluthge, Smirnov, Anton, Samaroo, Julian, Schloss, James, Wilcox, Lucas C, Byrne,
Simon, Waruszewski, Maciej, Ramadhan, Ali, , Meredith, Schaub, Simeon, Besard, Tim, Constantinou,
Navid C., Bolewski, Jake, Ng, Max, Arthur, Ben, Kawczynski, Charles, Hill, Chris, Rackauckas, Christopher,
Cook, James, Jinguo Liu, Schanen, Michel, Schulz, Oliver, , Oscar, Haraldsson, Páll, Arakaki, Takafumi, and
Chor, Tomas (2023). JuliaGPU/KernelAbstractions.jl: v0.9.2. doi:10.5281/ZENODO.7818509.
Churavy, Valentin, Godoy, William F, Bauer, Carsten, Ranocha, Hendrik, Schlottke-Lakemper, Michael,
Räss, Ludovic, Blaschke, Johannes, Giordano, Mosè, Schnetter, Erik, Omlin, Samuel, Vetter, Jeffrey S.,
and Edelman, Alan (2022).
Bridging HPC Communities through the Julia Programming Language.
doi:10.48550/ARXIV.2211.02740.
Costa,
Pedro
(2018).
A
FFT-based
finite-difference
solver
for
massively-parallel
direct
numeri-
cal simulations of turbulent flows.
Computers & Mathematics with Applications, 76(8):1853–1862.
doi:10.1016/j.camwa.2018.07.034.
10
Dabiri, John O, Colin, Sean P, Costello, John H, and Gharib, Morteza (2005). Flow patterns generated by
oblate medusan jellyfish: field measurements and laboratory analyses.
Journal of Experimental Biology,
208(7):1257–1265.
Dairay, Thibault, Lamballais, Eric, Laizet, Sylvain, and Vassilicos, John Christos (2017). Numerical dissipa-
tion vs. subgrid-scale modelling for large eddy simulation. Journal of Computational Physics, 337:252–274.
doi:10.1016/j.jcp.2017.02.035.
Davidon, William C (1991). Variable metric method for minimization. SIAM Journal on optimization, 1(1):1–
17.
Fischer, Paul, Kerkemeier, Stefan, Min, Misun, Lan, Yu-Hsiang, Phillips, Malachi, Rathnayake, Thilina,
Merzari, Elia, Tomboulides, Ananias, Karakus, Ali, Chalmers, Noel, and Warburton, Tim (2022).
NekRS, a GPU-accelerated spectral element Navier–Stokes solver.
Parallel Computing, 114:102982.
doi:10.1016/j.parco.2022.102982.
Font, Bernat, Alcántara-Ávila, Francisco, Rabault, Jean, Vinuesa, Ricardo, and Lehmkuhl, Oriol (2024). Active
flow control of a turbulent separation bubble through deep reinforcement learning.
Journal of Physics:
Conference Series, 2753(1):012022. doi:10.1088/1742-6596/2753/1/012022.
Font, Bernat, Weymouth, Gabriel D., Nguyen, Vinh-Tan, and Tutty, Owen R. (2021).
Deep learn-
ing of the spanwise-averaged Navier–Stokes equations.
Journal of Computational Physics, 434:110199.
doi:10.1016/j.jcp.2021.110199.
Gasparino, L., Spiga, F., and Lehmkuhl, O. (2024).
SOD2D: A GPU-enabled Spectral Finite Elements
Method for compressible scale-resolving simulations.
Computer Physics Communications, 297:109067.
doi:10.1016/j.cpc.2023.109067.
Guastoni, Luca, Rabault, Jean, Schlatter, Philipp, Azizpour, Hossein, and Vinuesa, Ricardo (2023).
Deep
reinforcement learning for turbulent drag reduction in channel flows.
The European Physical Journal E,
46(4). doi:10.1140/epje/s10189-023-00285-8.
Jofre, Lluís, Abdellatif, Ahmed, and Oyarzun, Guillermo (2023).
RHEA: an open-source Reproducible
Hybrid-architecture flow solver Engineered for Academia.
Journal of Open Source Software, 8(81):4637.
doi:10.21105/joss.04637.
Kempf, Daniel, Kurz, Marius, Blind, Marcel, Kopper, Patrick, Offenhäuser, Philipp, Schwarz, Anna, Starr,
Spencer, Keim, Jens, and Beck, Andrea (2024). GALÆXI: Solving complex compressible flows with high-
order discontinuous Galerkin methods on accelerator-based systems. doi:10.48550/ARXIV.2404.12703.
Kurz, Marius, Offenhäuser, Philipp, Viola, Dominic, Resch, Michael, and Beck, Andrea (2022). Relexi — A
scalable open source reinforcement learning framework for high-performance computing. Software Impacts,
14:100422. doi:10.1016/j.simpa.2022.100422.
Lauber, Marin, Weymouth, Gabriel D., and Limbert, Georges (2022).
Immersed boundary simula-
tions of flows driven by moving thin membranes.
Journal of Computational Physics, 457:111076.
doi:10.1016/j.jcp.2022.111076.
Lusher, David J., Jammy, Satya P., and Sandham, Neil D. (2021). OpenSBLI: Automated code-generation for
heterogeneous computing architectures applied to compressible fluid dynamics on structured grids. Computer
Physics Communications, 267:108063. doi:10.1016/j.cpc.2021.108063.
Maertens, A. P. and Weymouth, G. D. (2015).
Accurate Cartesian-grid simulations of near-body flows at
intermediate Reynolds numbers. Computer Methods in Applied Mechanics and Engineering, 283:106 – 129.
doi:https://doi.org/10.1016/j.cma.2014.09.007.
Margolin, L. G., Rider, W. J., and Grinstein, F. F. (2006). Modeling turbulent flow with implicit LES. Journal
of Turbulence, 7:N15. ISSN: 1468-5248, doi:10.1080/14685240500331595.
Rabault, Jean, Kuchta, Miroslav, Jensen, Atle, Réglade, Ulysse, and Cerardi, Nicolas (2019). Artificial neural
networks trained through deep reinforcement learning discover control strategies for active flow control.
Journal of Fluid Mechanics, 865:281–302. doi:10.1017/jfm.2019.62.
Ramadhan, Ali, Wagner, Gregory LeClaire, Hill, Chris, Campin, Jean-Michel, Churavy, Valentin, Besard,
Tim, Souza, Andre, Edelman, Alan, Ferrari, Raffaele, and Marshall, John (2020).
Oceananigans.jl:
Fast and friendly geophysical fluid dynamics on GPUs.
Journal of Open Source Software, 5(53):2018.
doi:10.21105/joss.02018.
11
Revels, J., Lubin, M., and Papamarkou, T. (2016).
Forward-mode automatic differentiation in Julia.
arXiv:1607.07892 [cs.MS]. url: https://arxiv.org/abs/1607.07892.
Rodriguez, Ivette, Borrel, Ricard, Lehmkuhl, Oriol, Perez Ssegarra, Carlos D., and Oliva, Assensi (2011).
Direct numerical simulation of the flow over a sphere at Re=3700. Journal of Fluid Mechanics, 679:263–287.
doi:10.1017/jfm.2011.136.
Romero, Joshua, Costa, Pedro, and Fatica, Massimiliano (2022). Distributed-memory simulations of turbulent
flows on modern GPU systems using an adaptive pencil decomposition library. In Proceedings of the Platform
for Advanced Scientific Computing Conference, PASC ’22. ACM. doi:10.1145/3539781.3539797.
Schulmeister, James C, Dahl, JM, Weymouth, GD, and Triantafyllou, MS (2017). Flow control with rotating
cylinders. Journal of Fluid Mechanics, 825:743–763.
Weymouth, G.D. and Yue, Dick K.P. (2011).
Boundary data immersion method for Cartesian-grid
simulations of fluid-body interaction problems.
Journal of Computational Physics, 230(16):6233–6247.
doi:10.1016/j.jcp.2011.04.022.
Weymouth, Gabriel D. (2022). Data-driven Multi-Grid solver for accelerated pressure projection. Computers
& Fluids, 246:105620. doi:10.1016/j.compfluid.2022.105620.
Weymouyh,
Gabriel D. and Lauber,
Marin (2023).
ParametricBodies.jl.
https://github.com/
WaterLily-jl/ParametricBodies.jl.
Witherden, F.D., Vermeire, B.C., and Vincent, P.E. (2015). Heterogeneous computing on mixed unstructured
grids with PyFR. Computers & Fluids, 120:173–186. doi:10.1016/j.compfluid.2015.07.016.
Yun, Giwoong, Kim, Dongjoo, and Choi, Haecheon (2006). Vortical structures behind a sphere at subcritical
Reynolds numbers. Physics of Fluids, 18(1). doi:10.1063/1.2166454.
Zurman-Nasution, Andhini N, Ganapathisubramani, Bharathram, and Weymouth, Gabriel D (2021).
Fin
sweep angle does not determine flapping propulsive performance. Journal of the Royal Society Interface,
18(178):20210174.
12
